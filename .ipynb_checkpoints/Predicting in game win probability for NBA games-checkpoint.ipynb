{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2359be",
   "metadata": {},
   "source": [
    "# Predicting in game win probabilities of NBA games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a2f3b",
   "metadata": {},
   "source": [
    "## Why?, How?, what else has been done?\n",
    "\n",
    "Before I moved to Canada I never really watched basketball live, mostly due to the time difference and me liking my sleep but when I did get the oppourtunity to move here in March/April 2019 I witnessed the Toronto Raptors make their championship run and ever since then i've been hooked. \n",
    "\n",
    "Naturally as a Data Scientist and someone who studied Maths at university I was drawn to the numbers side of things with basketball. Advanced stats, different ways of measuring impact, elo scores and other people's efforts working with basketball data showed me the other half of the sport. \n",
    "\n",
    "Seeing other peoples efforts really highlighted the amount of data that can be collected and analysed so I decided to do my own project with NBA data. Python has an excellent [API wrapper](https://github.com/swar/nba_api) around stats.nba.com with access to things I didn't even know were tracked. \n",
    "\n",
    "An interesting part of the api is that it provided live play by play data and I became interested in how nba games were predicted live."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd973dd",
   "metadata": {},
   "source": [
    "## Reserach / Review\n",
    "\n",
    "Looking around at what had been done I found a few blogs/articles/papers mostly related to other sports but some around basketball (NCAA and NBA).\n",
    "\n",
    "- [Bayesian approach to predicting football(not soccer) games](https://dtai.cs.kuleuven.be/sports/blog/a-bayesian-approach-to-in-game-win-probability) [1]\n",
    "\n",
    "- [Brian Burkes NFL forecasting](http://wagesofwins.com/2009/03/05/modeling-win-probability-for-a-college-basketball-game-a-guest-post-from-brian-burke/) [2]\n",
    "\n",
    "- [py-ball](https://github.com/basketballrelativity/py_ball) [3]\n",
    "\n",
    "- [inpredictable](http://stats.inpredictable.com/nba/wpBox_live.php) [4]\n",
    "\n",
    "\n",
    "Reading the [1] definitely cleared up that predicting football was a lot harder since a lot of games end in draws and there is a lot of infrequent scoring.\n",
    "\n",
    "[2] & [3] gave me the first step in order to build a model to predict games. The approach that was used here is to split the game into n-second intervals and then build a series of logisitc regression models, one for each interval. \n",
    "\n",
    "[4] gave me a sense of what other blogs were doing and something to compare my graphs too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e344ac",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52b6f3",
   "metadata": {},
   "source": [
    "As mentioned before, python has a GREAT wrapper for the stats.nba.com api again linked [here](https://github.com/swar/nba_api), which is worth checking out in your own time just to see the volume of data available to play with. But I wrote a simple script to collect all the playbyplay data for the last ~7 odd years. \n",
    "The problem is rate limits! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdcb202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to get all live playbyplay data\n",
    "from nba_api.stats.endpoints import leaguegamelog, playbyplayv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2640c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = [f'20{x}-{x+1}' for x in range(13,22)] #can put whatever range you want\n",
    "\n",
    "league_game_logs = []\n",
    "for season in seasons:\n",
    "    game_log = leaguegamelog.LeagueGameLog(season=season).get_data_frames()[0]\n",
    "    league_game_logs.append(game_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc1c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "league_game_log = pd.concat(league_game_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9025e9",
   "metadata": {},
   "source": [
    "#### Filtering for home games\n",
    "\n",
    "The reason i'm filtering for the home games below is because there can only be two outcomes in basketball, win or lose and so if you find the probability that the home team wins then you're done. The game log api wrapper returns game logs from both teams perspectives. Matchups with '@' in them are instances from the perspective of the away team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a95293",
   "metadata": {},
   "outputs": [],
   "source": [
    "league_game_log = league_game_log[~league_game_log['MATCHUP'].str.contains('@')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebaa252",
   "metadata": {},
   "source": [
    "once you get all the game logs you can go and retrieve all playbyplay data. Usually when there are no rate limits I spam the API (WITHIN REASON) using multiprocessing however since the NBA api has some serious API limits this is a slow burning job that'll take a few hours. I suggest you run this before you get on with something else and let it run in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc7eb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñè                                                                                                                                                                         | 14/10284 [00:09<1:52:42,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "pbp = []\n",
    "for game_id in tqdm(league_game_log.GAME_ID):\n",
    "    pbp.append(playbyplayv2.PlayByPlayV2(game_id).get_data_frames()[0])\n",
    "    sleep(0.2)\n",
    "    #to ensure over time we aren't spamming the api and hitting any rate limits, set it to whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65098575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(pbp)\n",
    "df.to_csv('bpbp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459fa30",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Featurizing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3720a6",
   "metadata": {},
   "source": [
    "okay we have our play by play data but now what? \n",
    "The first version of our model was to replicate what has already been done. So our current goal is to build 960 logistic regression models, one for each three second period.\n",
    "\n",
    "Since our play by play data is not uniformly generated i.e. records have times from 0-2880 but they are not uniformly spaced and definitely not every three seconds (the shot clock itself goes on for 24 seconds) so what can we do? \n",
    "\n",
    "As Data Scientists real world data is never going to be perfect, formatting, quality, frequency etc but we must do what we can with what we have. \n",
    "\n",
    "And so we need to make assumptions and preprocess our data accordingly. \n",
    "\n",
    "Assumptions we're going to make:\n",
    "- The state of the game is the same until the next play by play event i.e. if a play happened at 2700 and the score was 10-15 and the next play happened at 2680 and the score is now 12-15 then the time between 2700 and 2680 still has the score 10-15. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa376e6",
   "metadata": {},
   "source": [
    "The features we are aiming to produce are for the first iteration is\n",
    "\n",
    "- Whos got possesion? \n",
    "- Score difference\n",
    "- is it over time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf31e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whos got possession? \n",
    "df[['home_true', 'visitor_true']] = df[['HOMEDESCRIPTION','VISITORDESCRIPTION']].notnull().astype(int)\n",
    "\n",
    "df['block'] = df['HOMEDESCRIPTION'].str.contains(\"BLOCK\").fillna(False)\n",
    "df['steal'] = df['HOMEDESCRIPTION'].str.contains(\"STEAL\").fillna(False)\n",
    "\n",
    "def home_poss(d):\n",
    "    #logic follows that whenever the home description or visitor description is filled out then its that teams \n",
    "    #possession unless theres a block or steal!\n",
    "    if (d['home_true'] == 1) & (d['visitor_true']==0):\n",
    "        return 1\n",
    "    elif (d['home_true'] == 0) & (d['visitor_true']==1):\n",
    "        return 0\n",
    "    else:\n",
    "        if d['block'] or d['steal']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "df['home_poss'] = df.apply(home_poss, axis = 1)\n",
    "\n",
    "#get the score difference\n",
    "df['score'] = df.groupby('GAME_ID')['SCORE'].ffill().fillna('0-0')\n",
    "df[['away_score', 'home_score']] = df['score'].str.split('-', expand=True)\n",
    "df['diff'] = pd.to_numeric(df['home_score']) - pd.to_numeric(df['away_score'])\n",
    "\n",
    "#To see what period we're in, if we're in overtime it will show us what period of overtime we're in otherwise just 0\n",
    "#for regulation games\n",
    "df['OT_ind'] = (df['PERIOD']-4).clip(lower=0)\n",
    "\n",
    "\n",
    "#to find the time remaining\n",
    "from datetime import datetime\n",
    "\n",
    "def str_to_time(str1):\n",
    "    time_ = datetime.strptime(str1, \"%M:%S\")\n",
    "    return time_.second + time_.minute*60\n",
    "\n",
    "df['seconds'] = df['PCTIMESTRING'].apply(str_to_time)\n",
    "\n",
    "def find_seconds_left(x):\n",
    "    if x == 1:\n",
    "        return 3*720\n",
    "    elif x == 2: \n",
    "        return 2*720\n",
    "    elif x == 3:\n",
    "        return 720\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df['seconds_left_in_game_from_quarter'] = df['PERIOD'].apply(find_seconds_left)\n",
    "\n",
    "df['time_remaining'] = df['seconds'] + df['seconds_left_in_game_from_quarter']\n",
    "\n",
    "#Create our targets\n",
    "df['home_team_win'] = df.groupby('GAME_ID')['diff'].last().clip(lower=0, upper=1).rename('home_team_win')\n",
    "\n",
    "#final dataset subsetting for our relevant columns\n",
    "data = df[['GAME_ID','home_poss', 'diff', 'time_remaining', 'OT_ind']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d684a",
   "metadata": {},
   "source": [
    "So now we have our data in the format of \n",
    "\n",
    "GAME_ID | home_poss | diff | time_remaining | OT_ind | home_team_win\n",
    "\n",
    "But, how do we build our 960 models? especially since looking at our data shows that the play by plays recorded are definitely not uniformly distributed in three second intervals\n",
    "\n",
    "We have to do some clever indexing and fast filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids = list(df['GAME_ID'].sort_values().unique())\n",
    "iterables = [game_ids, list(range(2880,-1,-1))]\n",
    "countdown = pd.DataFrame(index = pd.MultiIndex.from_product(iterables, names=[\"GAME_ID\", \"time_remaining\"])).reset_index()\n",
    "\n",
    "final = countdown.merge(data, on = ['GAME_ID', 'time_remaining'], how='left').ffill()\n",
    "final = final[final['time_remaining'].isin(range(0,2883,3))]\n",
    "final.to_csv('modelling_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a32f6",
   "metadata": {},
   "source": [
    "Now we have our modelling data time to build our series of logisitc regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5363d93",
   "metadata": {},
   "source": [
    "# Model Building - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our train and test sets, our test set will be the most recent last 500 games\n",
    "test_games = df['GAME_ID'].unique()[-500:]\n",
    "\n",
    "test = df[df['GAME_ID'].isin(test_games)]\n",
    "train = df[~(df['GAME_ID'].isin(test_games))]\n",
    "\n",
    "train = train.drop('GAME_ID', axis = 1).set_index('time_remaining')\n",
    "test = test.set_index(['time_remaining', 'GAME_ID'])\n",
    "\n",
    "X = train.drop('home_team_win', axis = 1)\n",
    "y = train['home_team_win']\n",
    "\n",
    "#build our series of logistic regression models \n",
    "models = {}\n",
    "for time in X.index.unique():\n",
    "    model_temp = LogisticRegression()\n",
    "    X_temp = X.loc[time]\n",
    "    y_temp = y.loc[time]\n",
    "    model_temp.fit(X_temp.values, y_temp.values)\n",
    "    models[time] = model_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#create our test set predictions\n",
    "probs = []\n",
    "preds = []\n",
    "for time in test.index.levels[0]:\n",
    "    temp = test.loc[time].drop('home_team_win', axis = 1)\n",
    "    probs.append(pd.DataFrame(models[time].predict_proba(temp), columns = ['prob_home_lose', 'prob_home_win']))\n",
    "    preds.append(pd.DataFrame(models[time].predict(temp), columns=['preds']))\n",
    "\n",
    "probs_df = pd.concat(probs).reset_index(drop=True)\n",
    "preds_df = pd.concat(preds).reset_index(drop=True)\n",
    "\n",
    "test_df = pd.concat([test.reset_index(), probs_df, preds_df], axis = 1).set_index('GAME_ID').sort_index().sort_values(by=['OT_ind','time_remaining'], ascending=[True, False])\n",
    "\n",
    "test_df['time'] = 2880-test_df['time_remaining']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9e8f8",
   "metadata": {},
   "source": [
    "## Moving forward!\n",
    "\n",
    "I want to predict NBA games live, a lot of this on the fly data manipulation can make predictions slower and add a lot of over head to each computation so what can we do moving forward? \n",
    "\n",
    "- Include time as a feature\n",
    "- How can we incorporate more team specific information\n",
    "\n",
    "\n",
    "Ideas! \n",
    "- Use a neural network to learn features as we have a lot of tablular data\n",
    "- Incorporate 538's ELO score - this takes into account recent team performance, injuries etc and is managed externally, completely. Obviously composing all these complicated aspects of data into one metric we lose information however it is a good heuristic for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Elo Scores\n",
    "\n",
    "def read_url_to_csv(url):\n",
    "    r = requests.get(url)\n",
    "    data = io.StringIO(r.text)\n",
    "    df = pd.read_csv(data, sep=\",\")\n",
    "    return df\n",
    "\n",
    "elo_url = 'https://projects.fivethirtyeight.com/nba-model/nba_elo.csv'\n",
    "\n",
    "elo = read_url_to_csv(elo_url)\n",
    "elo = elo[elo['date'] > '2012-01-01']\n",
    "\n",
    "elo.loc[:, 'elo_difference'] = np.abs(elo['elo1_pre'] - elo['elo2_pre'])\n",
    "\n",
    "elo = elo[['date', 'team1', 'elo1_pre', 'elo2_pre', 'elo_difference']]\n",
    "\n",
    "elo['team1'] = elo['team1'].replace({'BRK':'BKN',\n",
    "                                     'PHO':'PHX',\n",
    "                                    'CHO':'CHA',})\n",
    "\n",
    "#To merge ELO scores with my games since game_id isn't a universal key amongst disparate basketball data sources\n",
    "\n",
    "all_games = league_game_log.copy()\n",
    "all_games[['Home', 'Away']] = all_games['MATCHUP'].str.split('vs.', expand=True)\n",
    "all_games['Home'] = all_games['Home'].str.strip()\n",
    "all_games['GAME_ID'] = all_games['GAME_ID'].astype(int)\n",
    "all_games['home_team_win'] = all_games['WL'].replace({'W':1, 'L':0})\n",
    "\n",
    "\n",
    "elo_w_game_ids = all_games.merge(elo, left_on=['GAME_DATE', 'Home'], right_on = ['date', 'team1'])\n",
    "#merge it with our 'not standardized' to three seconds data\n",
    "df = df.merge(elo_w_game_ids[['GAME_ID', 'elo1_pre', 'elo2_pre', 'elo_difference', 'home_team_win']], on = ['GAME_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_games = games[:7000]\n",
    "test_games = games[7000:]\n",
    "train = df[df['GAME_ID'].isin(train_games)].drop(['GAME_ID', 'elo_difference'], axis = 1)\n",
    "\n",
    "X_train = train.drop(['home_team_win'], axis = 1)\n",
    "y_train = train['home_team_win']\n",
    "\n",
    "\n",
    "test = df[df['GAME_ID'].isin(test_games)]\n",
    "\n",
    "X_test = test.drop(['home_team_win', 'GAME_ID'], axis = 1)\n",
    "y_test = test['home_team_win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32232a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8096a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                60        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229\n",
      "Trainable params: 229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 12)                84        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 253\n",
      "Trainable params: 253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 08:05:46.740678: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_wO_elo = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, activation='relu', input_shape=[4,]),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_wO_elo.compile(optimizer=tf.keras.optimizers.RMSprop(1e-3), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model_wO_elo.summary()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, activation='relu', input_shape=[6,]),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(1e-3), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0dd25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wO_elo = X_train.drop(['elo1_pre', 'elo2_pre'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wO_elo.fit(X_train_wO_elo.values,\n",
    "          y_train.values,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          epochs=15,\n",
    "          batch_size=512)\n",
    "\n",
    "model.fit(X_train.values,\n",
    "          y_train.values,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          epochs=15,\n",
    "          batch_size=512)\n",
    "\n",
    "model.save('../Models/TF_model_w_elo.h5')\n",
    "model_wO_elo.save('../Models/TF_model_wO_elo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, 'preds_w_elo'] = model.predict_on_batch(X_test[X_train.columns])\n",
    "\n",
    "test.loc[:, 'preds_wO_elo'] = model_wO_elo.predict_on_batch(X_test[X_train_wO_elo.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ea6187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15495321224308004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score_loss(test['home_team_win'], test['preds_w_elo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd596788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1654293641223685\n"
     ]
    }
   ],
   "source": [
    "brier_score_loss(test['home_team_win'], test['preds_wO_elo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d647c27",
   "metadata": {},
   "source": [
    "# Evaluating our model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
