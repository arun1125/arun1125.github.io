{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39455b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [3, 1, 2, 3, 6, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbc1f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corrupt_numbers(nums):\n",
    "    # TODO: Write your code here\n",
    "    \n",
    "    i = 0\n",
    "    dup_num = None\n",
    "    while i < len(nums):\n",
    "        if nums[i] != i+1:\n",
    "            j = nums[i] - 1\n",
    "            if nums[i] != nums[j]:\n",
    "                nums[i], nums[j] = nums[j], nums[i]\n",
    "            else:\n",
    "                dup_num = nums[i]\n",
    "                break\n",
    "        else:\n",
    "            i+=1\n",
    "            \n",
    "    missing_num = None\n",
    "    \n",
    "    i = 0\n",
    "    n = len(nums)\n",
    "    res = [-1]*(n+1)\n",
    "    \n",
    "    while i < len(nums):\n",
    "        j = nums[i] \n",
    "        res[j] = j\n",
    "        i+=1\n",
    "        \n",
    "    for k in range(len(res)):\n",
    "        if res[k] < 0:\n",
    "            missing_num = k\n",
    "            \n",
    "    return [dup_num, missing_num]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0bd24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corrupt_numbers_dave(nums):\n",
    "    #cyclic sort\n",
    "    #element by element comparison to find dup\n",
    "    #do the same -1 bullshit to find missing num\n",
    "    \n",
    "    \n",
    "    #cyclic sort\n",
    "    \n",
    "    curr_idx = 0 \n",
    "    while curr_idx < len(nums):\n",
    "        proper_idx = nums[curr_idx] - 1\n",
    "        \n",
    "        if nums[curr_idx] != nums[proper_idx]:\n",
    "            nums[curr_idx], nums[proper_idx] = nums[proper_idx], nums[curr_idx]\n",
    "        else:\n",
    "            curr_idx += 1\n",
    "            \n",
    "    for i in range(len(nums)):\n",
    "        if nums[i] != i+1:\n",
    "            return [nums[i], i+1]\n",
    "            \n",
    "    return [-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ea396af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_corrupt_numbers_dave(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15c4e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_smallest_missing_positive_number(nums):\n",
    "    curr_index = 0\n",
    "    \n",
    "    while curr_index < len(nums):\n",
    "        proper_index = nums[curr_index] - 1\n",
    "\n",
    "        \n",
    "        if nums[curr_index] > 0 and nums[curr_index] <= len(nums) and nums[curr_index]!=nums[proper_index]:\n",
    "            nums[curr_index], nums[proper_index] = nums[proper_index], nums[curr_index]\n",
    "        else:\n",
    "            curr_index += 1\n",
    "            \n",
    "    for i in range(len(nums)):\n",
    "        if nums[i] != i+1:\n",
    "            return i+1\n",
    "        \n",
    "    return len(nums) + 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee7fd794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_smallest_missing_positive_number([3, 2, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f1f6e",
   "metadata": {},
   "source": [
    "## Moving forward!\n",
    "\n",
    "I want to predict NBA games live, a lot of this on the fly data manipulation can make predictions slower and add a lot of over head to each computation so what can we do moving forward? \n",
    "\n",
    "- Include time as a feature\n",
    "- How can we incorporate more team specific information\n",
    "\n",
    "\n",
    "Ideas! \n",
    "- Use a neural network to learn features as we have a lot of tablular data\n",
    "- Incorporate 538's ELO score - this takes into account recent team performance, injuries etc and is managed externally, completely. Obviously composing all these complicated aspects of data into one metric we lose information however it is a good heuristic for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3914dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Elo Scores\n",
    "\n",
    "def read_url_to_csv(url):\n",
    "    r = requests.get(url)\n",
    "    data = io.StringIO(r.text)\n",
    "    df = pd.read_csv(data, sep=\",\")\n",
    "    return df\n",
    "\n",
    "elo_url = 'https://projects.fivethirtyeight.com/nba-model/nba_elo.csv'\n",
    "\n",
    "elo = read_url_to_csv(elo_url)\n",
    "elo = elo[elo['date'] > '2012-01-01']\n",
    "\n",
    "elo.loc[:, 'elo_difference'] = np.abs(elo['elo1_pre'] - elo['elo2_pre'])\n",
    "\n",
    "elo = elo[['date', 'team1', 'elo1_pre', 'elo2_pre', 'elo_difference']]\n",
    "\n",
    "elo['team1'] = elo['team1'].replace({'BRK':'BKN',\n",
    "                                     'PHO':'PHX',\n",
    "                                    'CHO':'CHA',})\n",
    "\n",
    "#To merge ELO scores with my games since game_id isn't a universal key amongst disparate basketball data sources\n",
    "\n",
    "all_games = league_game_log.copy()\n",
    "all_games[['Home', 'Away']] = all_games['MATCHUP'].str.split('vs.', expand=True)\n",
    "all_games['Home'] = all_games['Home'].str.strip()\n",
    "all_games['GAME_ID'] = all_games['GAME_ID'].astype(int)\n",
    "all_games['home_team_win'] = all_games['WL'].replace({'W':1, 'L':0})\n",
    "\n",
    "\n",
    "elo_w_game_ids = all_games.merge(elo, left_on=['GAME_DATE', 'Home'], right_on = ['date', 'team1'])\n",
    "#merge it with our 'not standardized' to three seconds data\n",
    "df = df.merge(elo_w_game_ids[['GAME_ID', 'elo1_pre', 'elo2_pre', 'elo_difference', 'home_team_win']], on = ['GAME_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76428409",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_games = games[:7000]\n",
    "test_games = games[7000:]\n",
    "train = df[df['GAME_ID'].isin(train_games)].drop(['GAME_ID', 'elo_difference'], axis = 1)\n",
    "\n",
    "X_train = train.drop(['home_team_win'], axis = 1)\n",
    "y_train = train['home_team_win']\n",
    "\n",
    "\n",
    "test = df[df['GAME_ID'].isin(test_games)]\n",
    "\n",
    "X_test = test.drop(['home_team_win', 'GAME_ID'], axis = 1)\n",
    "y_test = test['home_team_win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b8663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5780d88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                60        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229\n",
      "Trainable params: 229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 12)                84        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 253\n",
      "Trainable params: 253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 08:05:46.740678: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_wO_elo = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, activation='relu', input_shape=[4,]),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_wO_elo.compile(optimizer=tf.keras.optimizers.RMSprop(1e-3), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model_wO_elo.summary()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, activation='relu', input_shape=[6,]),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(1e-3), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wO_elo = X_train.drop(['elo1_pre', 'elo2_pre'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wO_elo.fit(X_train_wO_elo.values,\n",
    "          y_train.values,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          epochs=15,\n",
    "          batch_size=512)\n",
    "\n",
    "model.fit(X_train.values,\n",
    "          y_train.values,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          epochs=15,\n",
    "          batch_size=512)\n",
    "\n",
    "model.save('../Models/TF_model_w_elo.h5')\n",
    "model_wO_elo.save('../Models/TF_model_wO_elo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, 'preds_w_elo'] = model.predict_on_batch(X_test[X_train.columns])\n",
    "\n",
    "test.loc[:, 'preds_wO_elo'] = model_wO_elo.predict_on_batch(X_test[X_train_wO_elo.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98313fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15495321224308004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score_loss(test['home_team_win'], test['preds_w_elo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753c2157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1654293641223685\n"
     ]
    }
   ],
   "source": [
    "brier_score_loss(test['home_team_win'], test['preds_wO_elo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcb17c",
   "metadata": {},
   "source": [
    "# Evaluating our model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moving forward!\n",
    "\n",
    "I want to predict NBA games live, a lot of this on the fly data manipulation can make predictions slower and add a lot of over head to each computation so what can we do moving forward? \n",
    "\n",
    "- Include time as a feature\n",
    "- How can we incorporate more team specific information\n",
    "\n",
    "\n",
    "Ideas! \n",
    "- Use a neural network to learn features as we have a lot of tablular data\n",
    "- Incorporate 538's ELO score - this takes into account recent team performance, injuries etc and is managed externally, completely. Obviously composing all these complicated aspects of data into one metric we lose information however it is a good heuristic for now\n",
    "\n",
    "#Get Elo Scores\n",
    "\n",
    "def read_url_to_csv(url):\n",
    "    r = requests.get(url)\n",
    "    data = io.StringIO(r.text)\n",
    "    df = pd.read_csv(data, sep=\",\")\n",
    "    return df\n",
    "\n",
    "elo_url = 'https://projects.fivethirtyeight.com/nba-model/nba_elo.csv'\n",
    "\n",
    "elo = read_url_to_csv(elo_url)\n",
    "elo = elo[elo['date'] > '2012-01-01']\n",
    "\n",
    "elo.loc[:, 'elo_difference'] = np.abs(elo['elo1_pre'] - elo['elo2_pre'])\n",
    "\n",
    "elo = elo[['date', 'team1', 'elo1_pre', 'elo2_pre', 'elo_difference']]\n",
    "\n",
    "elo['team1'] = elo['team1'].replace({'BRK':'BKN',\n",
    "                                     'PHO':'PHX',\n",
    "                                    'CHO':'CHA',})\n",
    "\n",
    "#To merge ELO scores with my games since game_id isn't a universal key amongst disparate basketball data sources\n",
    "\n",
    "all_games = league_game_log.copy()\n",
    "all_games[['Home', 'Away']] = all_games['MATCHUP'].str.split('vs.', expand=True)\n",
    "all_games['Home'] = all_games['Home'].str.strip()\n",
    "all_games['GAME_ID'] = all_games['GAME_ID'].astype(int)\n",
    "all_games['home_team_win'] = all_games['WL'].replace({'W':1, 'L':0})\n",
    "\n",
    "\n",
    "elo_w_game_ids = all_games.merge(elo, left_on=['GAME_DATE', 'Home'], right_on = ['date', 'team1'])\n",
    "#merge it with our 'not standardized' to three seconds data\n",
    "df = df.merge(elo_w_game_ids[['GAME_ID', 'elo1_pre', 'elo2_pre', 'elo_difference', 'home_team_win']], on = ['GAME_ID'])\n",
    "\n",
    "train_games = games[:7000]\n",
    "test_games = games[7000:]\n",
    "train = df[df['GAME_ID'].isin(train_games)].drop(['GAME_ID', 'elo_difference'], axis = 1)\n",
    "\n",
    "X_train = train.drop(['home_team_win'], axis = 1)\n",
    "y_train = train['home_team_win']\n",
    "\n",
    "\n",
    "test = df[df['GAME_ID'].isin(test_games)]\n",
    "\n",
    "X_test = test.drop(['home_team_win', 'GAME_ID'], axis = 1)\n",
    "y_test = test['home_team_win']\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model_wO_elo = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, activation='relu', input_shape=[4,]),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_wO_elo.compile(optimizer=tf.keras.optimizers.RMSprop(1e-3), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model_wO_elo.summary()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, activation='relu', input_shape=[6,]),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(1e-3), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model.summary()\n",
    "\n",
    "X_train_wO_elo = X_train.drop(['elo1_pre', 'elo2_pre'], axis = 1)\n",
    "\n",
    "model_wO_elo.fit(X_train_wO_elo.values,\n",
    "          y_train.values,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          epochs=15,\n",
    "          batch_size=512)\n",
    "\n",
    "model.fit(X_train.values,\n",
    "          y_train.values,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          epochs=15,\n",
    "          batch_size=512)\n",
    "\n",
    "model.save('../Models/TF_model_w_elo.h5')\n",
    "model_wO_elo.save('../Models/TF_model_wO_elo.h5')\n",
    "\n",
    "test.loc[:, 'preds_w_elo'] = model.predict_on_batch(X_test[X_train.columns])\n",
    "\n",
    "test.loc[:, 'preds_wO_elo'] = model_wO_elo.predict_on_batch(X_test[X_train_wO_elo.columns])\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score_loss(test['home_team_win'], test['preds_w_elo'])\n",
    "\n",
    "brier_score_loss(test['home_team_win'], test['preds_wO_elo'])\n",
    "\n",
    "# Evaluating our model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
